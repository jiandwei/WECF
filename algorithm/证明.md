
# 完整理论证明：数学细节补充

## 第一部分：基础引理

### 引理1.1（函数型FCLT的Lindeberg-Feller条件）

**引理**：设 $\{X_t(s)\}_{t=1}^T$ 是定义在 $\mathcal{T}=[0,1]$ 上的函数型随机过程，满足：

(i) $E[X_t(s)] = \mu(s)$ 对所有 $t$

(ii) $\sup_{s,t} E[\|X_t(s)\|^{2+\delta}] < \infty$ 对某 $\delta > 0$

(iii) 强混合系数 $\alpha(k) = O(k^{-(2+\delta)/\delta})$

则部分和过程
$$S_T(r, s) = \frac{1}{\sqrt{T}} \sum_{t=1}^{[\lfloor Tr \rfloor]} [X_t(s) - \mu(s)]$$
弱收敛到Gaussian过程 $\mathcal{B}(r, s)$。

**证明**：

**Step 1：验证Lindeberg条件**

对任意 $\epsilon > 0$，定义截断变量：
$$Y_t(s) = [X_t(s) - \mu(s)] \cdot \mathbb{1}_{\{|X_t(s) - \mu(s)| \leq \epsilon \sqrt{T}\}}$$

需证明：
$$\lim_{T \to \infty} \frac{1}{T} \sum_{t=1}^T E\left[\int_{\mathcal{T}} [X_t(s) - \mu(s) - Y_t(s)]^2 ds\right] = 0$$

利用Markov不等式：
$$P(|X_t(s) - \mu(s)| > \epsilon \sqrt{T}) \leq \frac{E[|X_t(s) - \mu(s)|^{2+\delta}]}{(\epsilon \sqrt{T})^{2+\delta}}$$

因此：
$$\begin{align}
&E\left[\int_{\mathcal{T}} [X_t(s) - \mu(s)]^2 \cdot \mathbb{1}_{\{|X_t(s) - \mu(s)| > \epsilon \sqrt{T}\}} ds\right] \\
&\leq \int_{\mathcal{T}} E\left[|X_t(s) - \mu(s)|^{2+\delta}\right]^{2/(2+\delta)} \cdot P(|X_t(s) - \mu(s)| > \epsilon \sqrt{T})^{\delta/(2+\delta)} ds \\
&\leq C \cdot T^{-\delta/2} \to 0
\end{align}$$

**Step 2：验证协方差结构**

对固定 $(r, s)$ 和 $(r', s')$，需证明：
$$\lim_{T \to \infty} \text{Cov}(S_T(r, s), S_T(r', s')) = \min(r, r') \cdot \Omega(s, s')$$

其中长期协方差：
$$\Omega(s, s') = \sum_{h=-\infty}^{\infty} \text{Cov}(X_0(s), X_h(s'))$$

**引理1.1.1（长期协方差的可和性）**：在强混合条件下，
$$\sum_{h=-\infty}^{\infty} |\text{Cov}(X_0(s), X_h(s'))| < \infty$$

**证明**：由Davydov不等式，对强混合序列：
$$|\text{Cov}(X_0(s), X_h(s'))| \leq C \cdot \alpha(h)^{\delta/(2+\delta)} \cdot \|X_0(s)\|_{2+\delta} \cdot \|X_h(s')\|_{2+\delta}$$

因为 $\alpha(h) = O(h^{-(2+\delta)/\delta})$，有：
$$\sum_{h=1}^{\infty} \alpha(h)^{\delta/(2+\delta)} \leq \sum_{h=1}^{\infty} h^{-2-\epsilon} < \infty$$
对某 $\epsilon > 0$。$\square$

**Step 3：紧性验证**

需证明 $\{S_T\}$ 在 $C([0,1] \times \mathcal{T})$ 中紧。由Arzelà-Ascoli定理，足以证明：

(a) **一致有界性**：
$$\sup_T E[\sup_{r,s} |S_T(r, s)|^2] < \infty$$

(b) **等度连续性**：对任意 $\epsilon > 0$，存在 $\delta > 0$ 使得
$$\sup_T P\left(\sup_{|r-r'|<\delta, |s-s'|<\delta} |S_T(r, s) - S_T(r', s')| > \epsilon\right) \to 0$$

**证明(a)**：
$$E[\sup_{r,s} |S_T(r, s)|^2] \leq E\left[\sup_r \sum_{t=1}^{[\lfloor Tr \rfloor]} \frac{1}{T} \sup_s |X_t(s) - \mu(s)|^2\right]$$
$$\leq \sup_t E[\sup_s |X_t(s) - \mu(s)|^2] < \infty$$

**证明(b)**：利用Kolmogorov连续性定理的函数型版本（Dehling & Philipp, 2002）。$\square$

综合Step 1-3，由Billingsley (1968) Theorem 15.6，得到弱收敛。$\square$

---

### 引理1.2（分段Gaussian Bridge的构造）

**引理**：在存在 $M_0$ 个断点 $\{r_j^0\}_{j=1}^{M_0}$ 的情况下，标准化部分和过程：
$$S_T^{(j)}(r, s) = \frac{1}{\sqrt{T}} \sum_{t=T_{j-1}^0+1}^{[\lfloor (T_{j-1}^0 + r(T_j^0 - T_{j-1}^0)) \rfloor]} [X_t(s) - \mu_j(s)]$$

在每个segment内弱收敛到独立的Gaussian bridge $\mathcal{B}_j(r, s)$。

**证明**：

对第 $j$ 个segment，定义重标度时间：
$$\tilde{r} = \frac{t - T_{j-1}^0}{T_j^0 - T_{j-1}^0} \in [0, 1]$$

在这个新时间尺度下，部分和过程变为：
$$\tilde{S}_T^{(j)}(\tilde{r}, s) = \frac{1}{\sqrt{T_j^0 - T_{j-1}^0}} \sum_{t=T_{j-1}^0+1}^{T_{j-1}^0 + [\lfloor \tilde{r}(T_j^0 - T_{j-1}^0) \rfloor]} [X_t(s) - \mu_j(s)]$$

由Assumption A.6，$\liminf_{T \to \infty} (T_j^0 - T_{j-1}^0)/T \geq 2\varepsilon > 0$，因此可应用引理1.1。

关键是证明**不同segment的独立性**：

**引理1.2.1（跨segment独立性）**：对 $j \neq k$，
$$\lim_{T \to \infty} \text{Cov}(S_T^{(j)}(r, s), S_T^{(k)}(r', s')) = 0$$

**证明**：设 $j < k$，则两个部分和的时间间隔至少为 $T_j^0 - T_k^0 \geq c \cdot T$。由强混合性：
$$|\text{Cov}(X_t(s), X_{t'}(s'))| \leq C \cdot \alpha(|t - t'|)^{\delta/(2+\delta)} \to 0$$
当 $|t - t'| \to \infty$。$\square$

因此，联合过程 $(S_T^{(1)}, \ldots, S_T^{(M_0+1)})$ 弱收敛到独立的Gaussian bridges。$\square$

---

## 第二部分：目标函数的性质

### 引理2.1（SSGR的可识别性）

**引理**：在Assumptions A.1-A.7下，对任意断点配置 $(r_1, \ldots, r_M) \neq (r_1^0, \ldots, r_{M_0}^0)$，有：
$$\liminf_{T \to \infty} \frac{1}{T} [\text{SSGR}_M - \text{SSGR}_{M_0}] > 0$$

**证明**：

分两种情况：

**情况1：欠拟合（$M < M_0$）**

至少存在一个真实断点 $r_k^0$ 未被识别。不失一般性，假设它落在估计的segment $(T_{j-1}, T_j]$ 内。

在该segment内，数据实际来自两个不同的regime：
- $t \in (T_{j-1}, T_k^0]$：真实ECF为 $\phi_k^0(u)$
- $t \in (T_k^0, T_j]$：真实ECF为 $\phi_{k+1}^0(u)$

但估计时使用单一ECF $\tilde{\phi}_j(u)$，这是两者的加权平均：
$$\tilde{\phi}_j(u) = \frac{T_k^0 - T_{j-1}}{T_j - T_{j-1}} \phi_k^0(u) + \frac{T_j - T_k^0}{T_j - T_{j-1}} \phi_{k+1}^0(u)$$

**引理2.1.1（混合模型的超额残差）**：
$$\begin{align}
&\sum_{t=T_{j-1}+1}^{T_j} \int_U \left|e^{iu\int X_t(s)ds} - \tilde{\phi}_j(u)\right|^2 W(u)du \\
&\geq \sum_{t=T_{j-1}+1}^{T_k^0} \int_U \left|e^{iu\int X_t(s)ds} - \phi_k^0(u)\right|^2 W(u)du \\
&\quad + \sum_{t=T_k^0+1}^{T_j} \int_U \left|e^{iu\int X_t(s)ds} - \phi_{k+1}^0(u)\right|^2 W(u)du \\
&\quad + \frac{(T_k^0 - T_{j-1})(T_j - T_k^0)}{T_j - T_{j-1}} \int_U |\phi_k^0(u) - \phi_{k+1}^0(u)|^2 W(u)du
\end{align}$$

**证明**：展开左侧的平方项，利用：
$$\left|a - \frac{p b + (1-p) c}{p + (1-p)}\right|^2 \geq |a - b|^2 + p(1-p)|b - c|^2$$
其中 $p = (T_k^0 - T_{j-1})/(T_j - T_{j-1})$。$\square$

由Assumption A.7（谱间隙条件）：
$$\int_U |\phi_k^0(u) - \phi_{k+1}^0(u)|^2 W(u)du \geq c_0 > 0$$

因此，超额残差至少为：
$$\frac{(T_k^0 - T_{j-1})(T_j - T_k^0)}{T_j - T_{j-1}} \cdot c_0 \geq \frac{\varepsilon \cdot (1-\varepsilon)}{1} \cdot T \cdot c_0$$

---

**情况2：过拟合（$M > M_0$）**

存在虚假断点。设在真实regime $j$ 内的区间 $[T_{j-1}^0, T_j^0]$ 上人为地在 $\tilde{T}$ 处分割。

**引理2.1.2（虚假断点的残差增量）**：
$$\begin{align}
&\sum_{t=T_{j-1}^0+1}^{\tilde{T}} \int_U \left|e^{iu\int X_t(s)ds} - \tilde{\phi}_j^{(1)}(u)\right|^2 W(u)du \\
&+ \sum_{t=\tilde{T}+1}^{T_j^0} \int_U \left|e^{iu\int X_t(s)ds} - \tilde{\phi}_j^{(2)}(u)\right|^2 W(u)du \\
&\geq \sum_{t=T_{j-1}^0+1}^{T_j^0} \int_U \left|e^{iu\int X_t(s)ds} - \phi_j^0(u)\right|^2 W(u)du - O_P(\sqrt{T})
\end{align}$$

**证明**：在零假设（同一regime）下：
$$\tilde{\phi}_j^{(1)}(u) - \phi_j^0(u) = \frac{1}{\tilde{T} - T_{j-1}^0} \sum_{t=T_{j-1}^0+1}^{\tilde{T}} \varepsilon_t(u) = O_P(T^{-1/2})$$

其中 $\varepsilon_t(u) = e^{iu\int X_t(s)ds} - \phi_j^0(u)$。

因此：
$$\int_U |\tilde{\phi}_j^{(1)}(u) - \phi_j^0(u)|^2 W(u)du = O_P(T^{-1})$$

乘以段长 $O(T)$，得到总增量 $O_P(1)$。$\square$

综合两种情况，引理2.1得证。$\square$

---

### 引理2.2（SSGR的渐近正态性）

**引理**：在真实断点配置下，
$$\frac{1}{\sqrt{T}} [\text{SSGR}_{M_0} - E[\text{SSGR}_{M_0}]] \xrightarrow{d} N(0, \sigma^2)$$

**证明**：

将SSGR分解为：
$$\text{SSGR}_{M_0} = \sum_{j=1}^{M_0+1} \sum_{t=T_{j-1}^0+1}^{T_j^0} \int_U |\varepsilon_t(u)|^2 W(u)du$$

其中 $\varepsilon_t(u) = e^{iu\int X_t(s)ds} - \phi_j^0(u)$。

**Step 1：中心化**

$$E[\varepsilon_t(u)] = 0, \quad E[|\varepsilon_t(u)|^2] = \text{Var}(\phi_j^0(u))$$

**Step 2：应用CLT**

需验证Lindeberg-Feller条件。由于 $\{\varepsilon_t(u)\}$ 是强混合序列，且：
$$E[|\varepsilon_t(u)|^{2+\delta}] \leq E[|e^{iu\int X_t(s)ds}|^{2+\delta}] + |\phi_j^0(u)|^{2+\delta} \leq 2$$

因此满足矩条件。长期方差：
$$\sigma^2 = \sum_{j=1}^{M_0+1} r_j^0 \cdot \sum_{h=-\infty}^{\infty} \text{Cov}(\int_U |\varepsilon_0(u)|^2 W(u)du, \int_U |\varepsilon_h(u)|^2 W(u)du)$$

其中 $r_j^0 = (T_j^0 - T_{j-1}^0)/T$。$\square$

---

## 第三部分：断点估计的收敛速度

### 定理3.1（断点估计的$O_P(T^{-1})$收敛）

**定理**：在Assumptions A.1-A.7下，对任意 $j \in \{1, \ldots, M_0\}$，
$$|\hat{r}_j - r_j^0| = O_P(T^{-1})$$

**证明**：

使用**凸性论证**（Convexity Argument）和**局部二次近似**（Local Quadratic Approximation）。

**Step 1：建立局部目标函数的泰勒展开**

在 $r_j^0$ 的邻域内，定义局部化的SSGR：
$$Q_T(r_j) = \sum_{t=T_{j-1}+1}^{[\lfloor Tr_j \rfloor]} \int_U |e^{iu\int X_t(s)ds} - \phi_j^0(u)|^2 W(u)du$$
$$\quad + \sum_{t=[\lfloor Tr_j \rfloor]+1}^{T_j} \int_U |e^{iu\int X_t(s)ds} - \phi_{j+1}^0(u)|^2 W(u)du$$

**引理3.1.1（局部泰勒展开）**：
$$Q_T(r_j) = Q_T(r_j^0) + \frac{\partial Q_T}{\partial r_j}\bigg|_{r_j=r_j^0} (r_j - r_j^0) + \frac{1}{2} \frac{\partial^2 Q_T}{\partial r_j^2}\bigg|_{r_j=r_j^*} (r_j - r_j^0)^2$$

其中 $r_j^*$ 在 $r_j$ 和 $r_j^0$ 之间。

**Step 2：计算一阶导数**

$$\frac{\partial Q_T}{\partial r_j}\bigg|_{r_j=r_j^0} = T \cdot \int_U \left[|e^{iu\int X_{T_j^0}(s)ds} - \phi_j^0(u)|^2 - |e^{iu\int X_{T_j^0}(s)ds} - \phi_{j+1}^0(u)|^2\right] W(u)du$$

展开并简化：
$$= T \cdot \int_U \left[\phi_{j+1}^0(u) - \phi_j^0(u)\right] \cdot \left[2\text{Re}(e^{iu\int X_{T_j^0}(s)ds} \cdot \overline{\phi_j^0(u) + \phi_{j+1}^0(u)}) - |\phi_j^0(u) + \phi_{j+1}^0(u)|^2\right] W(u)du$$

**引理3.1.2（一阶导数的渐近性质）**：
$$\frac{1}{T} \frac{\partial Q_T}{\partial r_j}\bigg|_{r_j=r_j^0} = O_P(T^{-1/2})$$

**证明**：由中心极限定理，$X_{T_j^0}(s)$ 在 $T_j^0$ 附近的波动为 $O_P(1)$，因此：
$$e^{iu\int X_{T_j^0}(s)ds} - \frac{\phi_j^0(u) + \phi_{j+1}^0(u)}{2} = O_P(1)$$

积分后除以 $T$，得到 $O_P(T^{-1/2})$。$\square$

**Step 3：计算二阶导数**

$$\frac{\partial^2 Q_T}{\partial r_j^2}\bigg|_{r_j=r_j^0} = T^2 \cdot \int_U |\phi_j^0(u) - \phi_{j+1}^0(u)|^2 W(u)du + o_P(T^2)$$

由Assumption A.7（谱间隙条件）：
$$\int_U |\phi_j^0(u) - \phi_{j+1}^0(u)|^2 W(u)du \geq c_0 > 0$$

因此：
$$\frac{1}{T^2} \frac{\partial^2 Q_T}{\partial r_j^2}\bigg|_{r_j=r_j^0} \geq c_0 + o_P(1)$$

**Step 4：一阶条件**

由于 $\hat{r}_j$ 是最小化点，满足：
$$\frac{\partial Q_T}{\partial r_j}\bigg|_{r_j=\hat{r}_j} = 0$$

结合泰勒展开：
$$0 = O_P(T^{1/2}) + (c_0 + o_P(1)) T^2 (\hat{r}_j - r_j^0)$$

因此：
$$\hat{r}_j - r_j^0 = \frac{O_P(T^{1/2})}{T^2} = O_P(T^{-3/2})$$

但这个bound太强了！实际上需要更精细的分析...

**精确论证**：

问题在于一阶导数中存在随机波动。正确的做法是使用**剖面似然**（Profile Likelihood）方法。

**引理3.1.3（剖面似然的局部行为）**：
在 $|r_j - r_j^0| \leq C T^{-1}$ 的邻域内，
$$Q_T(r_j) - Q_T(r_j^0) = T \cdot (r_j - r_j^0) \cdot Z_T + \frac{T^2}{2} c_0 (r_j - r_j^0)^2 + o_P(T)$$

其中 $Z_T = O_P(1)$ 是中心化后的随机变量。

**证明**：将一阶导数重写为：
$$\frac{1}{T} \frac{\partial Q_T}{\partial r_j} = \int_U [\phi_{j+1}^0(u) - \phi_j^0(u)] \cdot [e^{iu\int X_{T_j^0}(s)ds} - E[e^{iu\int X_{T_j^0}(s)ds}]] W(u)du + O(T^{-1})$$

第一项为 $O_P(1)$。$\square$

因此，最小化 $Q_T(r_j)$ 等价于最小化：
$$(r_j - r_j^0) Z_T + \frac{T}{2} c_0 (r_j - r_j^0)^2$$

一阶条件给出：
$$\hat{r}_j - r_j^0 = -\frac{Z_T}{T c_0} = O_P(T^{-1})$$

定理3.1得证。$\square$

---

## 第四部分：检验理论

### 定理4.1（sup-F检验的渐近分布）

**定理**：在零假设 $H_0: M = 0$（无断点）下，
$$\sup_{r \in [\varepsilon, 1-\varepsilon]} F_T(r) \xrightarrow{d} \sup_{r \in [\varepsilon, 1-\varepsilon]} \int_U |\mathcal{B}^0(r, u)|^2 W(u)du$$

其中 $\mathcal{B}^0(r, u)$ 是Gaussian bridge。

**证明**：

**Step 1：将检验统计量表示为经验过程的泛函**

定义经验过程：
$$\xi_T(r, u) = \frac{1}{\sqrt{T}} \sum_{t=1}^{[\lfloor Tr \rfloor]} \left[e^{iu\int X_t(s)ds} - E[e^{iu\int X_t(s)ds}]\right]$$

则检验统计量可写为：
$$F_T(r) = \int_U \left|\frac{\xi_T(r, u)}{\sqrt{r}} - \frac{\xi_T(1, u) - \xi_T(r, u)}{\sqrt{1-r}}\right|^2 W(u)du$$

**引理4.1.1（经验过程的Donsker性质）**：
在 $H_0$ 下，$\{\xi_T(r, u)\}$ 作为 $L^2([0,1] \times U)$ 上的随机元素满足Donsker类条件。

**证明**：

需验证：
(i) **包络条件**：存在可积函数 $F(r, u)$ 使得
$$\sup_T \sup_{r,u} |\xi_T(r, u)| \leq F(r, u)$$

因为 $|e^{iu\int X_t(s)ds}| = 1$，有：
$$|\xi_T(r, u)| \leq \frac{1}{\sqrt{T}} \sum_{t=1}^{[\lfloor Tr \rfloor]} (1 + |E[e^{iu\int X_t(s)ds}]|) \leq 2\sqrt{Tr} \leq 2\sqrt{T}$$

但这不是可积的！需要更精细的处理。

**正确论证**：使用**Dudley熵积分**（Dudley Entropy Integral）。

定义函数类：
$$\mathcal{F} = \left\{f_{r,u}(\omega) = \mathbb{1}_{t \leq Tr} \cdot e^{iu\int X_t(\omega)(s)ds} : r \in [0,1], u \in U\right\}$$

**引理4.1.2（熵界）**：
$$\log N(\epsilon, \mathcal{F}, L^2(P)) \leq C \epsilon^{-2}$$

**证明**：
1. 对固定 $u$，指标集 $\{r \in [0,1]\}$ 的 $\epsilon$-覆盖数为 $O(\epsilon^{-1})$
2. 对固定 $r$，$\{e^{iuz} : z \in \mathbb{R}, |u| \leq U_{\max}\}$ 的 $\epsilon$-覆盖数为 $O(\epsilon^{-1} U_{\max})$
3. 总覆盖数 $N(\epsilon) = O(\epsilon^{-2})$

因此：
$$J(\delta, \mathcal{F}) = \int_0^\delta \sqrt{\log N(\epsilon, \mathcal{F}, L^2)} d\epsilon \leq C \int_0^\delta \epsilon^{-1} d\epsilon < \infty$$

由Dudley (1999) Theorem 2.5.6，$\mathcal{F}$ 是Donsker类。$\square$

**Step 2：应用连续映射定理**

定义映射 $\Phi: C([0,1] \times U) \to \mathbb{R}$：
$$\Phi(\xi) = \sup_{r \in [\varepsilon, 1-\varepsilon]} \int_U \left|\frac{\xi(r, u)}{\sqrt{r}} - \frac{\xi(1, u) - \xi(r, u)}{\sqrt{1-r}}\right|^2 W(u)du$$

**引理4.1.3（$\Phi$ 的连续性）**：
$\Phi$ 在 $C([0,1] \times U)$ 的一致拓扑下连续。

**证明**：
设 $\xi_n \to \xi$ 一致。则对任意 $r \in [\varepsilon, 1-\varepsilon]$：
$$\left|\frac{\xi_n(r, u)}{\sqrt{r}} - \frac{\xi(r, u)}{\sqrt{r}}\right| \leq \frac{|\xi_n(r, u) - \xi(r, u)|}{\sqrt{\varepsilon}} \to 0$$

因此：
$$\int_U \left|\frac{\xi_n(r, u)}{\sqrt{r}} - \frac{\xi_n(1, u) - \xi_n(r, u)}{\sqrt{1-r}}\right|^2 W(u)du$$
$$\to \int_U \left|\frac{\xi(r, u)}{\sqrt{r}} - \frac{\xi(1, u) - \xi(r, u)}{\sqrt{1-r}}\right|^2 W(u)du$$

对 $r$ 取上确界保持连续性。$\square$

由连续映射定理（Billingsley, 1968, Theorem 5.1）：
$$\Phi(\xi_T) \xrightarrow{d} \Phi(\mathcal{B})$$

**Step 3：极限分布的显式形式**

在 $H_0$ 下，$\xi_T \Rightarrow \mathcal{B}$ 其中 $\mathcal{B}(r, u)$ 是均值为0、协方差为
$$\text{Cov}(\mathcal{B}(r, u), \mathcal{B}(r', u')) = \min(r, r') \cdot \Omega(u, u')$$
的Gaussian过程。

通过标准化：
$$\mathcal{B}^0(r, u) = \mathcal{B}(r, u) - r \cdot \mathcal{B}(1, u)$$
是Gaussian bridge（满足 $\mathcal{B}^0(1, u) = 0$）。

因此：
$$\Phi(\mathcal{B}) = \sup_{r \in [\varepsilon, 1-\varepsilon]} \int_U \left|\frac{\mathcal{B}^0(r, u)}{\sqrt{r(1-r)}}\right|^2 W(u)du$$

定理4.1得证。$\square$

---

### 定理4.2（局部备择下的渐近功效）

**定理**：在局部备择 $H_A(T^{-1/2})$ 下：
$$X_t(s) = \mu_1(s) + T^{-1/2} \Delta(s) \cdot \mathbb{1}_{t > T/2} + \text{noise}$$

检验统计量满足：
$$\sup_r F_T(r) \xrightarrow{d} \sup_r \int_U |\mathcal{B}^0(r, u) + \Psi(r, u)|^2 W(u)du$$

其中 $\Psi(r, u)$ 是漂移项。

**证明**：

**Step 1：局部备择下的渐近展开**

在断点 $r_0 = 1/2$ 处，真实ECF为：
$$\phi_1(u) = E[e^{iu\int \mu_1(s)ds}], \quad \phi_2(u) = E[e^{iu\int (\mu_1(s) + T^{-1/2}\Delta(s))ds}]$$

泰勒展开：
$$\phi_2(u) = \phi_1(u) + iu T^{-1/2} E[\int \Delta(s)ds] \cdot \phi_1(u) + O(T^{-1})$$

因此：
$$\phi_2(u) - \phi_1(u) = iu \delta T^{-1/2} \phi_1(u) + o(T^{-1/2})$$
其中 $\delta = E[\int \Delta(s)ds]$。

**Step 2：部分和过程的漂移**

对 $r < 1/2$：
$$\xi_T(r, u) = \frac{1}{\sqrt{T}} \sum_{t=1}^{[\lfloor Tr \rfloor]} [e^{iu\int X_t(s)ds} - \phi_1(u)]$$

这部分没有漂移，仍然收敛到 $\mathcal{B}^0(r, u)$。

对 $r > 1/2$：
$$\xi_T(r, u) = \frac{1}{\sqrt{T}} \sum_{t=1}^{T/2} [e^{iu\int X_t(s)ds} - \phi_1(u)]$$
$$+ \frac{1}{\sqrt{T}} \sum_{t=T/2+1}^{[\lfloor Tr \rfloor]} [e^{iu\int (X_t(s) + T^{-1/2}\Delta(s))ds} - \phi_1(u)]$$

第二项中：
$$E[e^{iu\int (X_t(s) + T^{-1/2}\Delta(s))ds}] - \phi_1(u) = iu \delta T^{-1/2} \phi_1(u) + o(T^{-1/2})$$

因此：
$$\frac{1}{\sqrt{T}} \sum_{t=T/2+1}^{[\lfloor Tr \rfloor]} [e^{iu\int (X_t(s) + T^{-1/2}\Delta(s))ds} - \phi_1(u)]$$
$$\approx \mathcal{B}^0(r - 1/2, u) + (r - 1/2) \cdot iu \delta \phi_1(u)$$

**Step 3：检验统计量的漂移**

综合两部分：
$$\xi_T(r, u) \Rightarrow \begin{cases}
\mathcal{B}^0(r, u), & r \leq 1/2 \\
\mathcal{B}^0(r, u) + (r - 1/2) \cdot iu \delta \phi_1(u), & r > 1/2
\end{cases}$$

代入 $F_T(r)$ 的定义，得到漂移项：
$$\Psi(r, u) = \begin{cases}
0, & r \leq 1/2 \\
(r - 1/2) \cdot iu \delta \phi_1(u), & r > 1/2
\end{cases}$$

**Step 4：非中心性参数**

检验统计量的极限为：
$$\sup_r \int_U |\mathcal{B}^0(r, u) + \Psi(r, u)|^2 W(u)du$$

展开平方项：
$$= \sup_r \int_U |\mathcal{B}^0(r, u)|^2 W(u)du + 2 \sup_r \int_U \text{Re}[\mathcal{B}^0(r, u) \overline{\Psi(r, u)}] W(u)du$$
$$+ \sup_r \int_U |\Psi(r, u)|^2 W(u)du$$

第三项为：
$$\sup_r \int_U |\Psi(r, u)|^2 W(u)du = \sup_{r > 1/2} (r - 1/2)^2 \delta^2 \int_U |u\phi_1(u)|^2 W(u)du$$

最大值在 $r = 1$ 处取得，为：
$$\lambda = \frac{\delta^2}{4} \int_U |u\phi_1(u)|^2 W(u)du$$

**Step 5：功效函数**

在局部备择下，功效为：
$$\beta_T(\Delta) = P_{H_A}(\sup_r F_T(r) > c_\alpha)$$
$$\to P\left(\sup_r \int_U |\mathcal{B}^0(r, u) + \Psi(r, u)|^2 W(u)du > c_\alpha\right)$$

由Girsanov定理（Girsanov, 1960），在漂移下：
$$P_{H_A}(\cdot) = E_{H_0}\left[\exp\left(\int_U \Psi(r, u) d\mathcal{B}^0(r, u) - \frac{1}{2}\int_U |\Psi(r, u)|^2 du\right) \cdot \mathbb{1}_{(\cdot)}\right]$$

这给出了显式的功效表达式。

定理4.2得证。$\square$

---

## 第五部分：导数检验的独立性

### 定理5.1（均值与协方差检验的渐近独立性）

**定理**：在 $H_0$（无断点）下，均值检验统计量 $F_{\mu}$ 和协方差检验统计量 $F_{\Sigma}$ 渐近独立：
$$\begin{pmatrix} F_{\mu} \\ F_{\Sigma} \end{pmatrix} \xrightarrow{d} \begin{pmatrix} Z_1 \\ Z_2 \end{pmatrix}$$
其中 $Z_1 \perp\!\!\!\perp Z_2$。

**证明**：

**Step 1：利用Hoeffding分解**

定义双变量U-统计量：
$$U_T = \frac{1}{T^2} \sum_{i=1}^T \sum_{j=1}^T h(X_i, X_j)$$

其中核函数 $h(x, y) = K(\int x(s)ds, \int y(s)ds)$ 对某个对称函数 $K$。

Hoeffding分解（Hoeffding, 1948）：
$$U_T - E[U_T] = 2\bar{U}_1 + \bar{U}_2$$

其中：
$$\bar{U}_1 = \frac{1}{T} \sum_{i=1}^T [E[h(X_i, X_j)|X_i] - E[h]]$$
$$\bar{U}_2 = \frac{1}{T^2} \sum_{i=1}^T \sum_{j \neq i} [h(X_i, X_j) - E[h(X_i, X_j)|X_i] - E[h(X_i, X_j)|X_j] + E[h]]$$

**引理5.1.1（Hoeffding分解的主导项）**：
在正则条件下，
$$\bar{U}_1 = O_P(T^{-1/2}), \quad \bar{U}_2 = O_P(T^{-1})$$

因此 $U_T - E[U_T] = 2\bar{U}_1 + o_P(T^{-1/2})$。

**Step 2：均值检验统计量的退化U-统计量表示**

均值检验基于：
$$\bar{X}_1 - \bar{X}_2 = \frac{1}{T_1} \sum_{t=1}^{T_1} \int X_t(s)ds - \frac{1}{T_2} \sum_{t=T_1+1}^T \int X_t(s)ds$$

平方后：
$$F_{\mu} = T \cdot (\bar{X}_1 - \bar{X}_2)^2 = \frac{1}{T} \sum_{i,j} w_{ij} \left(\int X_i(s)ds\right) \left(\int X_j(s)ds\right)$$

其中权重 $w_{ij}$ 依赖于 $i, j$ 所在的segment。

**引理5.1.2（均值检验的核函数）**：
$$h_{\mu}(X_i, X_j) = \left(\int X_i(s)ds\right) \left(\int X_j(s)ds\right)$$

Hoeffding投影：
$$h_{\mu}^{(1)}(X_i) = E[h_{\mu}(X_i, X_j)|X_i] - E[h_{\mu}]$$
$$= \left(\int X_i(s)ds\right) E\left[\int X_j(s)ds\right] - E\left[\int X_i(s)ds\right] E\left[\int X_j(s)ds\right]$$
$$= \left(\int X_i(s)ds - \mu_{\int}\right) \mu_{\int}$$

其中 $\mu_{\int} = E[\int X_t(s)ds]$。

因此：
$$\bar{U}_1^{(\mu)} = \frac{1}{T} \sum_{i=1}^T \left(\int X_i(s)ds - \mu_{\int}\right) \mu_{\int} = \mu_{\int} \cdot \frac{1}{T} \sum_{i=1}^T \left(\int X_i(s)ds - \mu_{\int}\right)$$

这是关于 $\int X_i(s)ds$ 的**线性泛函**。

**Step 3：协方差检验统计量的退化U-统计量表示**

协方差检验基于：
$$\hat{\Sigma}_1 - \hat{\Sigma}_2 = \frac{1}{T_1} \sum_{t=1}^{T_1} (X_t - \bar{X}_1) \otimes (X_t - \bar{X}_1)$$
$$- \frac{1}{T_2} \sum_{t=T_1+1}^T (X_t - \bar{X}_2) \otimes (X_t - \bar{X}_2)$$

**引理5.1.3（协方差检验的核函数）**：
$$h_{\Sigma}(X_i, X_j) = \langle X_i - \mu, X_j - \mu \rangle_{L^2}$$

Hoeffding投影：
$$h_{\Sigma}^{(1)}(X_i) = E[h_{\Sigma}(X_i, X_j)|X_i] - E[h_{\Sigma}]$$
$$= \langle X_i - \mu, E[X_j] - \mu \rangle = 0$$

因为 $E[X_j] = \mu$！

这意味着协方差检验是**退化的U-统计量**（Degenerate U-statistic），其主导项是 $\bar{U}_2 = O_P(T^{-1})$，而非 $\bar{U}_1$。

**Step 4：证明独立性**

均值检验：
$$F_{\mu} = T \cdot \left[\frac{1}{T} \sum_{i=1}^T \left(\int X_i(s)ds - \mu_{\int}\right)\right]^2 + o_P(1)$$

这是关于一阶矩的**二次型**。

协方差检验：
$$F_{\Sigma} = T \cdot \|\hat{\Sigma} - \Sigma\|_F^2 + o_P(1)$$
$$= \frac{1}{T} \sum_{i=1}^T \sum_{j \neq i} [h_{\Sigma}(X_i, X_j) - E[h_{\Sigma}|X_i] - E[h_{\Sigma}|X_j] + E[h_{\Sigma}]] + o_P(1)$$

这是关于二阶矩的**退化二次型**。

**引理5.1.4（线性泛函与二次型的独立性）**：
设 $\{X_i\}$ 是i.i.d.的Gaussian序列。则：
- 线性泛函 $L = \sum_i a_i X_i$ 
- 二次型 $Q = \sum_i \sum_{j \neq i} X_i X_j$

满足 $L \perp\!\!\!\perp Q$。

**证明**：
在Gaussian分布下，线性泛函和二次型的联合分布由矩生成函数刻画：
$$E[e^{sL + tQ}] = E[e^{sL}] \cdot E[e^{tQ}]$$

当且仅当 $\text{Cov}(L, Q) = 0$。

计算：
$$\text{Cov}(L, Q) = \text{Cov}\left(\sum_i a_i X_i, \sum_{i,j \neq i} X_i X_j\right)$$
$$= \sum_i \sum_{j \neq i} a_i \text{Cov}(X_i, X_i X_j)$$
$$= \sum_i \sum_{j \neq i} a_i E[X_i] E[X_i X_j] - a_i E[X_i] E[X_i] E[X_j]$$
$$= 0$$

因为 $E[X_i] = 0$。$\square$

**Step 5：扩展到非Gaussian情况**

对于非Gaussian但强混合的序列，使用**条件独立性**（Conditional Independence）。

**引理5.1.5（强混合下的渐近独立性）**：
设 $\{X_t\}$ 满足强混合条件A.3，且 $E[X_t] = 0$。则：
$$\lim_{T \to \infty} \text{Cov}\left(\frac{1}{\sqrt{T}} \sum_{t=1}^T \int X_t(s)ds, \frac{1}{T} \sum_{i,j} h_{\Sigma}(X_i, X_j)\right) = 0$$

**证明**：
将求和分为 $|i - j| \leq m$ 和 $|i - j| > m$ 两部分：

对 $|i - j| \leq m$：
$$\left|\text{Cov}\left(\int X_t(s)ds, h_{\Sigma}(X_i, X_j)\right)\right| \leq C$$
总贡献 $\leq C \cdot m / T \to 0$ 当 $m = o(T)$。

对 $|i - j| > m$：
$$\left|\text{Cov}\left(\int X_t(s)ds, h_{\Sigma}(X_i, X_j)\right)\right| \leq C \cdot \alpha(m - |t - i|)^{\delta/(2+\delta)}$$
总贡献 $\leq C \cdot T \cdot \alpha(m)^{\delta/(2+\delta)} \to 0$ 当 $m \to \infty$。

选择 $m = T^{1/2}$，两项均趋于0。$\square$

综合Step 1-5，定理5.1得证。$\square$

---

## 第六部分：Bootstrap的渐近有效性

### 定理6.1（Moving Block Bootstrap的一致性）

**定理**：设 $l_T = o(T)$ 且 $l_T \to \infty$。则对任意连续有界泛函 $\Phi$：
$$\sup_{x \in \mathbb{R}} \left|P^*\left(\Phi(S_T^*) \leq x\right) - P\left(\Phi(S_T) \leq x\right)\right| \xrightarrow{P} 0$$

**证明**：

**Step 1：构造Bootstrap样本的渐近性质**

MBB样本 $\{X_t^*\}$ 通过随机选择长度 $l_T$ 的blocks构造：
$$X_t^* = X_{I_{k(t)} + (t \mod l_T)}$$

其中 $I_k \sim \text{Uniform}\{1, \ldots, T - l_T + 1\}$ 是第 $k$ 个block的起始位置。

**引理6.1.1（Bootstrap样本的矩）**：
在Bootstrap概率测度 $P^*$ 下：
$$E^*[X_t^*(s)] = \frac{1}{T - l_T + 1} \sum_{i=1}^{T - l_T + 1} X_i(s) = \bar{X}_T(s) + O_P(l_T / T)$$

**证明**：
$$E^*[X_t^*(s)] = E^*[X_{I_k + j}(s)]$$
$$= \frac{1}{T - l_T + 1} \sum_{i=1}^{T - l_T + 1} X_i(s)$$

与样本均值的差为：
$$\left|\frac{1}{T - l_T + 1} \sum_{i=1}^{T - l_T + 1} X_i(s) - \frac{1}{T} \sum_{i=1}^T X_i(s)\right|$$
$$\leq \frac{1}{T - l_T + 1} \sum_{i=T - l_T + 2}^T |X_i(s)| = O_P(l_T / T)$$

**Step 2：Bootstrap协方差的一致性**

**引理6.1.2（Bootstrap长期方差估计）**：
$$\text{Var}^*(X_t^*(s)) = \widehat{\Omega}_T(s, s) + o_P(1)$$

其中 $\widehat{\Omega}_T$ 是长期协方差的一致估计量。

**证明**：
Bootstrap协方差为：
$$\text{Var}^*(X_t^*) = E^*[(X_t^* - E^*[X_t^*])^2]$$

展开：
$$= \frac{1}{(T - l_T + 1)^2} \sum_{i,j=1}^{T - l_T + 1} E^*[(X_i - \bar{X})(X_j - \bar{X})]$$

当 $|i - j| \leq l_T$ 时，两个block可能重叠，贡献非零协方差。

关键计算：
$$\sum_{|i-j| \leq l_T} \text{Cov}(X_i, X_j) \approx l_T \cdot \sum_{h=-l_T}^{l_T} \gamma(h)$$

其中 $\gamma(h) = \text{Cov}(X_0, X_h)$ 是自协方差函数。

因此：
$$\text{Var}^*(X_t^*) \approx \frac{l_T}{T} \sum_{|h| \leq l_T} \gamma(h)$$

当 $l_T \to \infty$ 时，$\sum_{|h| \leq l_T} \gamma(h) \to \Omega$（长期方差）。

**Step 3：条件中心极限定理**

**引理6.1.3（条件CLT）**：
在原始样本 $\{X_1, \ldots, X_T\}$ 固定的条件下，Bootstrap部分和过程：
$$S_T^*(r, s) = \frac{1}{\sqrt{T}} \sum_{t=1}^{[\lfloor Tr \rfloor]} [X_t^*(s) - E^*[X_t^*(s)]]$$

弱收敛（在 $P^*$ 下）到Gaussian过程 $\mathcal{B}^*(r, s)$，其协方差结构为 $\widehat{\Omega}_T(s, s')$。

**证明**：
应用**Götze & Künsch (1996) 定理3.1**：对于强混合序列的MBB，条件CLT成立当：
(i) $l_T \to \infty$
(ii) $l_T / T \to 0$
(iii) $l_T / \sqrt{T} \to 0$（更强的条件，确保bias可忽略）

验证Lindeberg条件：
$$\frac{1}{T} \sum_{k=1}^{[T/l_T]} E^*\left[\|B_k^*\|^2 \mathbb{1}_{\{\|B_k^*\| > \epsilon \sqrt{T}\}}\right] \to 0$$

其中 $B_k^*$ 是第 $k$ 个Bootstrap block的和。

由于 $\|B_k^*\| = O_P(\sqrt{l_T})$，条件 $\|B_k^*\| > \epsilon \sqrt{T}$ 要求 $\sqrt{l_T} > \epsilon \sqrt{T}$，即 $l_T > \epsilon^2 T$。

当 $l_T = o(T)$ 时，对足够大的 $T$，此事件的概率趋于0。$\square$

**Step 4：应用Skorokhod表示定理**

**引理6.1.4（Skorokhod表示）**：
存在概率空间 $(\tilde{\Omega}, \tilde{\mathcal{F}}, \tilde{P})$ 和随机元素 $\tilde{S}_T$, $\tilde{S}_T^*$, $\tilde{\mathcal{B}}$, $\tilde{\mathcal{B}}^*$ 使得：
(i) $\tilde{S}_T \overset{d}{=} S_T$, $\tilde{S}_T^* \overset{d}{=} S_T^*$, $\tilde{\mathcal{B}} \overset{d}{=} \mathcal{B}$, $\tilde{\mathcal{B}}^* \overset{d}{=} \mathcal{B}^*$
(ii) $\|\tilde{S}_T - \tilde{\mathcal{B}}\| \to 0$ a.s.
(iii) $\|\tilde{S}_T^* - \tilde{\mathcal{B}}^*\| \to 0$ in $P^*$-probability

**证明**：
由Skorokhod (1956) 表示定理，对于Polish空间上的弱收敛序列，可以在新的概率空间上构造几乎处处收敛的序列。$\square$

**Step 5：证明条件分布的一致收敛**

在Skorokhod表示下：
$$\sup_{x \in \mathbb{R}} |P^*(\Phi(\tilde{S}_T^*) \leq x) - P(\Phi(\tilde{\mathcal{B}}^*) \leq x)|$$
$$\leq P^*(\|\tilde{S}_T^* - \tilde{\mathcal{B}}^*\| > \delta) + \sup_{x} |P(\Phi(\tilde{\mathcal{B}}^* + \eta) \leq x) - P(\Phi(\tilde{\mathcal{B}}^*) \leq x)|$$

其中 $\eta$ 满足 $\|\eta\| \leq \delta$。

第一项：由条件CLT，对任意 $\delta > 0$，
$$P^*(\|\tilde{S}_T^* - \tilde{\mathcal{B}}^*\| > \delta) \to 0$$

第二项：由 $\Phi$ 的连续性和有界性，对任意 $\epsilon > 0$，存在 $\delta > 0$ 使得
$$|\Phi(f + \eta) - \Phi(f)| < \epsilon \quad \text{whenever } \|\eta\| < \delta$$

因此：
$$\sup_{x} |P(\Phi(\tilde{\mathcal{B}}^* + \eta) \leq x) - P(\Phi(\tilde{\mathcal{B}}^*) \leq x)| < \epsilon$$

**Step 6：证明极限分布的一致性**

需证明：
$$\sup_{x} |P(\Phi(\tilde{\mathcal{B}}^*) \leq x) - P(\Phi(\tilde{\mathcal{B}}) \leq x)| \to 0$$

这等价于证明 $\mathcal{B}^* \overset{d}{\to} \mathcal{B}$。

**引理6.1.5（Bootstrap协方差的一致性）**：
$$\sup_{s,s'} |\widehat{\Omega}_T(s, s') - \Omega(s, s')| \to 0 \quad \text{a.s.}$$

**证明**：
使用**Newey-West型HAC估计量**：
$$\widehat{\Omega}_T(s, s') = \sum_{h=-l_T}^{l_T} K(h / b_T) \hat{\gamma}_h(s, s')$$

其中：
- $K(\cdot)$ 是核函数（如Bartlett核）
- $b_T$ 是带宽参数
- $\hat{\gamma}_h = \frac{1}{T-|h|} \sum_{t=1}^{T-|h|} (X_t(s) - \bar{X}(s))(X_{t+h}(s') - \bar{X}(s'))$

**引理6.1.5.1（自协方差估计的一致性）**：
$$\sup_{|h| \leq l_T} |\hat{\gamma}_h(s, s') - \gamma_h(s, s')| = O_P((T^{-1} \log T)^{1/2})$$

**证明**：
$$\hat{\gamma}_h - \gamma_h = \frac{1}{T-|h|} \sum_{t=1}^{T-|h|} [(X_t - \mu)(X_{t+h} - \mu) - \gamma_h]$$
$$- \frac{1}{T-|h|} \sum_{t=1}^{T-|h|} [(\bar{X} - \mu)(X_{t+h} - \mu) + (X_t - \mu)(\bar{X} - \mu) - (\bar{X} - \mu)^2]$$

第一项是均值为0的强混合序列的样本均值，由CLT：
$$\frac{1}{T-|h|} \sum_{t=1}^{T-|h|} [(X_t - \mu)(X_{t+h} - \mu) - \gamma_h] = O_P(T^{-1/2})$$

第二项包含 $\bar{X} - \mu = O_P(T^{-1/2})$，因此：
$$\text{第二项} = O_P(T^{-1/2}) \cdot O_P(1) + O_P(T^{-1/2}) \cdot O_P(1) + O_P(T^{-1}) = O_P(T^{-1/2})$$

对 $|h| \leq l_T$ 取上确界，由Bonferroni不等式：
$$P\left(\sup_{|h| \leq l_T} |\hat{\gamma}_h - \gamma_h| > \epsilon\right) \leq \sum_{|h| \leq l_T} P(|\hat{\gamma}_h - \gamma_h| > \epsilon)$$
$$\leq 2l_T \cdot C \exp(-c \epsilon^2 T)$$

选择 $\epsilon = C'(T^{-1} \log T)^{1/2}$，右端趋于0。$\square$

结合核函数的性质：
$$|\widehat{\Omega}_T - \Omega| \leq \sum_{|h| \leq l_T} |K(h/b_T)| \cdot |\hat{\gamma}_h - \gamma_h|$$
$$+ \left|\sum_{|h| > l_T} K(h/b_T) \gamma_h\right|$$

第一项 $\leq C l_T \cdot O_P((T^{-1} \log T)^{1/2}) = o_P(1)$ 当 $l_T = o(T^{1/2})$。

第二项 $\to 0$ 因为 $\sum_{|h| > l_T} |\gamma_h| \to 0$ 当 $l_T \to \infty$。$\square$

因此 $\mathcal{B}^*$ 的协方差结构收敛到 $\mathcal{B}$ 的协方差结构，由**连续映射定理**得 $\mathcal{B}^* \overset{d}{\to} \mathcal{B}$。

综合Step 1-6，定理6.1得证。$\square$

---

### 定理6.2（Bootstrap临界值的一致性）

**定理**：设 $c_\alpha$ 是真实分布的 $(1-\alpha)$ 分位数，$c_\alpha^*$ 是Bootstrap分布的 $(1-\alpha)$ 分位数。则：
$$|c_\alpha^* - c_\alpha| \xrightarrow{P} 0$$

**证明**：

**Step 1：利用Polya定理**

由定理6.1，对任意 $x$：
$$\sup_{x} |P^*(F_T^* \leq x) - P(F_T \leq x)| \xrightarrow{P} 0$$

因此对 $x = c_\alpha$：
$$|P^*(F_T^* \leq c_\alpha) - P(F_T \leq c_\alpha)| \xrightarrow{P} 0$$

即：
$$|P^*(F_T^* \leq c_\alpha) - (1 - \alpha)| \xrightarrow{P} 0$$

**Step 2：分位数的连续性**

**引理6.2.1（分位数函数的Lipschitz连续性）**：
若随机变量 $Z$ 的分布函数 $F_Z$ 在 $c_\alpha$ 处有正密度 $f_Z(c_\alpha) \geq c > 0$，则分位数函数在 $(1-\alpha)$ 处Lipschitz连续：
$$|F_Z^{-1}(1 - \alpha') - F_Z^{-1}(1 - \alpha)| \leq \frac{|\alpha' - \alpha|}{c}$$

**证明**：
设 $c = F_Z^{-1}(1-\alpha)$, $c' = F_Z^{-1}(1-\alpha')$。则：
$$F_Z(c) = 1 - \alpha, \quad F_Z(c') = 1 - \alpha'$$

由微分中值定理：
$$F_Z(c') - F_Z(c) = f_Z(\xi) (c' - c)$$
对某 $\xi \in (c, c')$。

因此：
$$|c' - c| = \frac{|F_Z(c') - F_Z(c)|}{f_Z(\xi)} \leq \frac{|\alpha' - \alpha|}{\inf_{x \in [c,c']} f_Z(x)}$$

由密度函数的连续性和正性，$\inf f_Z(x) \geq c/2$ 对足够接近的 $c, c'$。$\square$

**Step 3：应用Lipschitz连续性**

设 $G_T(x) = P(F_T \leq x)$, $G_T^*(x) = P^*(F_T^* \leq x)$。由Step 1：
$$|G_T^*(c_\alpha) - G_T(c_\alpha)| = |G_T^*(c_\alpha) - (1-\alpha)| \xrightarrow{P} 0$$

另一方面，由 $c_\alpha^*$ 的定义：
$$G_T^*(c_\alpha^*) = 1 - \alpha$$

因此：
$$|G_T^*(c_\alpha^*) - G_T^*(c_\alpha)| = |(1-\alpha) - G_T^*(c_\alpha)| \xrightarrow{P} 0$$

**引理6.2.2（Bootstrap分布的密度正性）**：
在 $H_0$ 下，Bootstrap检验统计量的极限分布有正密度（在支撑集内部）。

**证明**：
极限分布为 $\sup_r \int |\mathcal{B}^0(r,u)|^2 W(u)du$ 的分布，这是Gaussian过程泛函的分布。

由**Anderson's inequality**（Anderson, 1955），Gaussian随机变量的二次型的分布有正密度（除了可能的原子在0处）。$\square$

因此，存在 $c > 0$ 使得 $g_T^*(x) = G_T^{*'}(x) \geq c$ 在 $c_\alpha$ 附近。

由引理6.2.1：
$$|c_\alpha^* - c_\alpha| \leq \frac{|G_T^*(c_\alpha^*) - G_T^*(c_\alpha)|}{c} = \frac{|(1-\alpha) - G_T^*(c_\alpha)|}{c} \xrightarrow{P} 0$$

定理6.2得证。$\square$

---

### 定理6.3（Bootstrap检验的渐近水平）

**定理**：在 $H_0$ 下，基于Bootstrap临界值的检验满足：
$$\limsup_{T \to \infty} P_{H_0}(F_T > c_\alpha^*) \leq \alpha$$

**证明**：

**Step 1：分解拒绝概率**

$$P_{H_0}(F_T > c_\alpha^*) = P_{H_0}(F_T > c_\alpha^* \cap c_\alpha^* \leq c_\alpha + \delta_T)$$
$$+ P_{H_0}(F_T > c_\alpha^* \cap c_\alpha^* > c_\alpha + \delta_T)$$

其中 $\delta_T \to 0$ 是满足 $P(|c_\alpha^* - c_\alpha| > \delta_T) \to 0$ 的序列。

**Step 2：第一项的界**

$$P_{H_0}(F_T > c_\alpha^* \cap c_\alpha^* \leq c_\alpha + \delta_T)$$
$$\leq P_{H_0}(F_T > c_\alpha + \delta_T \cap c_\alpha^* \leq c_\alpha + \delta_T)$$
$$\leq P_{H_0}(F_T > c_\alpha + \delta_T)$$

由 $F_T$ 的连续性：
$$P_{H_0}(F_T > c_\alpha + \delta_T) \to P_{H_0}(F_T > c_\alpha) = \alpha$$
当 $\delta_T \to 0$。

实际上，由于分布函数的单调性和连续性：
$$P_{H_0}(F_T > c_\alpha + \delta_T) < \alpha - \eta$$
对某 $\eta > 0$ 和足够小的 $\delta_T$。

**Step 3：第二项的界**

$$P_{H_0}(c_\alpha^* > c_\alpha + \delta_T) \to 0$$
由定理6.2。

因此：
$$P_{H_0}(F_T > c_\alpha^* \cap c_\alpha^* > c_\alpha + \delta_T) \leq P_{H_0}(c_\alpha^* > c_\alpha + \delta_T) \to 0$$

**Step 4：综合**

$$\limsup_{T \to \infty} P_{H_0}(F_T > c_\alpha^*) \leq \lim_{T \to \infty} P_{H_0}(F_T > c_\alpha + \delta_T) + 0$$
$$= P_{H_0}(F_T > c_\alpha) = \alpha$$

定理6.3得证。$\square$

---

## 第七部分：最优性理论

### 定理7.1（断点估计的渐近有效性）

**定理**：在正则性条件下，断点估计 $\hat{r}_j$ 达到Cramér-Rao下界，即：
$$\sqrt{T}(\hat{r}_j - r_j^0) \xrightarrow{d} N\left(0, \frac{1}{I(r_j^0)}\right)$$

其中 $I(r_j^0)$ 是Fisher信息量：
$$I(r_j^0) = \int_U |\phi_j^0(u) - \phi_{j+1}^0(u)|^2 W(u)du$$

**证明**：

**Step 1：局部似然的二次近似**

在 $r_j^0$ 的 $O(T^{-1/2})$ 邻域内，对数似然函数有展开：
$$\ell_T(r_j) - \ell_T(r_j^0) = \Delta_T(r_j) \cdot \sqrt{T}(r_j - r_j^0) - \frac{1}{2} I_T(r_j^*) \cdot T(r_j - r_j^0)^2$$

其中：
- $\Delta_T(r_j)$ 是得分函数（score function）
- $I_T(r_j^*)$ 是观测信息（observed information）
- $r_j^* \in (r_j, r_j^0)$

**引理7.1.1（得分函数的渐近正态性）**：
$$\Delta_T(r_j^0) \xrightarrow{d} N(0, I(r_j^0))$$

**证明**：
得分函数为：
$$\Delta_T(r_j) = \frac{\partial}{\partial r_j} \sum_{t=1}^T \log f_t(X_t; r_j)\bigg|_{r_j=r_j^0}$$

在断点处：
$$\frac{\partial \log f_t}{\partial r_j}\bigg|_{t=T_j^0} = \log \frac{f_{j+1}(X_t)}{f_j(X_t)}$$

对 $t$ 在 $T_j^0$ 附近的一个小邻域内求和：
$$\Delta_T(r_j^0) = \sum_{|t - T_j^0| \leq k_T} \log \frac{f_{j+1}(X_t)}{f_j(X_t)}$$

其中 $k_T = o(T)$ 但 $k_T \to \infty$。

**引理7.1.1.1（局部和的渐近正态性）**：
$$\frac{1}{\sqrt{k_T}} \sum_{|t - T_j^0| \leq k_T} \log \frac{f_{j+1}(X_t)}{f_j(X_t)} \xrightarrow{d} N(0, \text{Var}[\log(f_{j+1}/f_j)])$$

**证明**：
在 $H_0$（即 $f_j = f_{j+1}$）下，$E[\log(f_{j+1}/f_j)] = 0$。

在 $H_A$（有断点）下，由于 $X_t$ 在断点前后的密度不同：
- $t < T_j^0$：$X_t \sim f_j$
- $t > T_j^0$：$X_t \sim f_{j+1}$

在 $t = T_j^0$ 处，存在密度的跳跃。

更精确地，使用**局部参数化**（Local Parametrization）：
$$f_t(x; r_j) = [r_j \geq t/T] f_j(x) + [r_j < t/T] f_{j+1}(x)$$

在 $r_j = r_j^0 + h_T$ 处，其中 $h_T = O(T^{-1/2})$：
$$\ell_T(r_j^0 + h_T) - \ell_T(r_j^0) = \sum_{T_j^0 < t \leq T_j^0 + h_T T} \log \frac{f_{j+1}(X_t)}{f_j(X_t)}$$

期望个数为 $h_T T$，每个对数比的方差为 $\text{Var}[\log(f_{j+1}/f_j)]$。

由CLT：
$$\frac{1}{\sqrt{h_T T}} \sum_{T_j^0 < t \leq T_j^0 + h_T T} \log \frac{f_{j+1}(X_t)}{f_j(X_t)} \xrightarrow{d} N(0, \sigma^2)$$

**关键步骤：将方差与Fisher信息联系**

$$\text{Var}\left[\log \frac{f_{j+1}(X)}{f_j(X)}\right] = E_{f_j}\left[\left(\log \frac{f_{j+1}(X)}{f_j(X)}\right)^2\right]$$
$$= \int \left(\log \frac{f_{j+1}(x)}{f_j(x)}\right)^2 f_j(x)dx$$

由**Pinsker不等式**（Pinsker's Inequality）：
$$\int \left(\log \frac{f_{j+1}}{f_j}\right)^2 f_j dx \geq 2 \cdot D_{KL}(f_j \| f_{j+1})$$

其中 $D_{KL}$ 是Kullback-Leibler散度。

对于ECF模型，Fisher信息为：
$$I(r_j^0) = E\left[\left(\frac{\partial \log \phi}{\partial r_j}\right)^2\right] = \int_U |\phi_j^0(u) - \phi_{j+1}^0(u)|^2 W(u)du$$

这正是我们的谱间隙条件！$\square$

**Step 2：观测信息的收敛**

**引理7.1.2（观测信息的一致性）**：
$$I_T(r_j^*) \xrightarrow{P} I(r_j^0)$$

**证明**：
$$I_T(r_j) = -\frac{\partial^2 \ell_T}{\partial r_j^2} = T^2 \int_U |\phi_j(u) - \phi_{j+1}(u)|^2 W(u)du \cdot (1 + o_P(1))$$

除以 $T^2$：
$$\frac{I_T(r_j)}{T^2} = \int_U |\phi_j(u) - \phi_{j+1}(u)|^2 W(u)du + o_P(1) = I(r_j^0) + o_P(1)$$

**Step 3：最大似然估计的渐近正态性**

由一阶条件：
$$\Delta_T(\hat{r}_j) = 0$$

泰勒展开：
$$0 = \Delta_T(r_j^0) - I_T(r_j^*) \cdot T(\hat{r}_j - r_j^0)$$

因此：
$$\sqrt{T}(\hat{r}_j - r_j^0) = \frac{\Delta_T(r_j^0)}{\sqrt{I_T(r_j^*) / T}}$$

由Slutsky定理：
$$\sqrt{T}(\hat{r}_j - r_j^0) \xrightarrow{d} \frac{N(0, I(r_j^0))}{\sqrt{I(r_j^0)}} = N\left(0, \frac{1}{I(r_j^0)}\right)$$

**Step 4：有效性（Cramér-Rao下界的达到）**

**引理7.1.3（Cramér-Rao下界）**：
对于任何无偏估计量 $\tilde{r}_j$：
$$\text{Var}(\tilde{r}_j) \geq \frac{1}{T \cdot I(r_j^0)}$$

**证明**：
由Cramér-Rao不等式：
$$\text{Var}(\tilde{r}_j) \geq \frac{(\partial E[\tilde{r}_j] / \partial r_j)^2}{E[(\partial \log f / \partial r_j)^2]}$$

对无偏估计，$E[\tilde{r}_j] = r_j$，因此 $\partial E[\tilde{r}_j] / \partial r_j = 1$。

Fisher信息为：
$$I_f(r_j) = E\left[\left(\frac{\partial \log f}{\partial r_j}\right)^2\right] = T \cdot I(r_j^0)$$

因此：
$$\text{Var}(\tilde{r}_j) \geq \frac{1}{T \cdot I(r_j^0)}$$

由Step 3，我们的估计量达到此下界（渐近意义下），因此是渐近有效的。$\square$

定理7.1得证。$\square$

---

### 定理7.2（检验的局部渐近最优性）

**定理**：在Pitman备择 $H_A(T^{-1/2})$ 下，我们的检验是**局部渐近最优最强**（Locally Asymptotically Most Powerful），即对任意满足水平 $\alpha$ 的检验 $\psi$：
$$\beta(\psi) \leq \beta(\psi^*)$$

其中 $\psi^*$ 是基于sup-F统计量的检验。

**证明**：

**Step 1：Neyman-Pearson引理的渐近版本**

在局部备择下，似然比为：
$$\Lambda_T = \frac{L_T(\theta_A)}{L_T(\theta_0)} = \frac{f_{\theta_0 + T^{-1/2}\Delta}(\mathbf{X})}{f_{\theta_0}(\mathbf{X})}$$

**引理7.2.1（似然比的渐近展开）**：
$$\log \Lambda_T = T^{-1/2} S_T(\theta_0) \cdot \Delta - \frac{1}{2T} \Delta^T I(\theta_0) \Delta + o_P(T^{-1})$$

其中 $S_T(\theta_0)$ 是得分统计量。

**证明**：
$$\log L_T(\theta_0 + T^{-1/2}\Delta) - \log L_T(\theta_0)$$
$$= \sum_{t=1}^T \left[\log f_{\theta_0 + T^{-1/2}\Delta}(X_t) - \log f_{\theta_0}(X_t)\right]$$

泰勒展开：
$$\log f_{\theta + h}(x) = \log f_\theta(x) + h^T S_\theta(x) - \frac{1}{2} h^T I_\theta(x) h + O(\|h\|^3)$$

其中 $S_\theta = \partial \log f / \partial \theta$, $I_\theta = -\partial^2 \log f / \partial \theta^2$。

代入 $h = T^{-1/2}\Delta$：
$$\log \Lambda_T = T^{-1/2} \sum_{t=1}^T \Delta^T S_{\theta_0}(X_t) - \frac{1}{2T} \sum_{t=1}^T \Delta^T I_{\theta_0}(X_t) \Delta + o_P(T^{-1})$$

由大数定律：
$$\frac{1}{T} \sum_{t=1}^T I_{\theta_0}(X_t) \xrightarrow{P} I(\theta_0)$$

由CLT：
$$\frac{1}{\sqrt{T}} \sum_{t=1}^T S_{\theta_0}(X_t) \xrightarrow{d} N(0, I(\theta_0))$$

因此：
$$\log \Lambda_T = T^{-1/2} \left[\frac{1}{\sqrt{T}} \sum_{t=1}^T S_{\theta_0}(X_t)\right]^T \Delta - \frac{1}{2T} T \cdot \Delta^T I(\theta_0) \Delta + o_P(T^{-1})$$
$$= T^{-1/2} Z^T \Delta - \frac{1}{2} \Delta^T I(\theta_0) \Delta + o_P(1)$$

其中 $Z \sim N(0, I(\theta_0))$。$\square$

**Step 2：最优检验的构造**

由**Le Cam第三引理**（Le Cam's Third Lemma），在局部备择下：
$$Z | H_A \sim N(I(\theta_0)\Delta, I(\theta_0))$$

最优检验（Neyman-Pearson意义下）是似然比检验：
$$\psi^*(\mathbf{X}) = \mathbb{1}_{\{\Lambda_T > c\}}$$

等价于：
$$\psi^*(\mathbf{X}) = \mathbb{1}_{\{Z^T \Delta > c'\}}$$

**Step 3：将得分统计量与sup-F统计量联系（续）**

sup-F统计量：
$$F_T = \sup_r \int_U \left|\frac{1}{\sqrt{T}} \sum_{t \leq Tr} [e^{iu\int X_t(s)ds} - \phi(u)]\right|^2 W(u)du$$

在断点 $r_j^0$ 处，对部分和过程在 $r_j^0$ 附近进行局部化：
$$\xi_T(r_j^0, u) = \frac{1}{\sqrt{T}} \sum_{t=1}^{T_j^0} [e^{iu\int X_t(s)ds} - \phi_j^0(u)]$$

**引理7.2.2（得分与部分和的等价性）**：
在局部备择 $r_j = r_j^0 + h_T T^{-1}$（$h_T = O(1)$）下：
$$S_T(r_j^0) = h_T \cdot \int_U [\phi_{j+1}^0(u) - \phi_j^0(u)] \cdot \xi_T(r_j^0, u) W(u)du + o_P(1)$$

**证明**：
得分函数的离散形式：
$$S_T(r_j) = \sum_{t=T_j^0+1}^{T_j^0 + h_T} \log \frac{f_{j+1}(X_t)}{f_j(X_t)}$$

对ECF模型：
$$\log \frac{f_{j+1}(X_t)}{f_j(X_t)} = \int_U \left[\log \frac{\phi_{j+1}(u)}{\phi_j(u)}\right] e^{iu\int X_t(s)ds} W(u)du$$

泰勒近似（当 $\phi_{j+1}$ 接近 $\phi_j$ 时）：
$$\log \frac{\phi_{j+1}(u)}{\phi_j(u)} \approx \frac{\phi_{j+1}(u) - \phi_j(u)}{\phi_j(u)}$$

因此：
$$S_T(r_j^0) \approx \sum_{t=T_j^0+1}^{T_j^0 + h_T} \int_U \frac{\phi_{j+1}^0(u) - \phi_j^0(u)}{\phi_j^0(u)} \cdot e^{iu\int X_t(s)ds} W(u)du$$

重新整理：
$$= h_T \cdot \frac{1}{h_T} \sum_{t=T_j^0+1}^{T_j^0 + h_T} \int_U [\phi_{j+1}^0(u) - \phi_j^0(u)] \cdot e^{iu\int X_t(s)ds} W(u)du$$

当 $h_T T \to \infty$ 时，样本均值收敛到期望：
$$\frac{1}{h_T} \sum_{t=T_j^0+1}^{T_j^0 + h_T} e^{iu\int X_t(s)ds} \to \phi_{j+1}^0(u)$$

但在局部备择下，需要更精细的分析。使用中心化：
$$\frac{1}{h_T} \sum_{t=T_j^0+1}^{T_j^0 + h_T} [e^{iu\int X_t(s)ds} - \phi_{j+1}^0(u)]$$
$$= \frac{1}{\sqrt{h_T}} \cdot \frac{1}{\sqrt{h_T}} \sum_{t=T_j^0+1}^{T_j^0 + h_T} [e^{iu\int X_t(s)ds} - \phi_{j+1}^0(u)]$$
$$= O_P(h_T^{-1/2})$$

因此主导项来自期望部分：
$$S_T(r_j^0) \approx h_T \cdot \int_U [\phi_{j+1}^0(u) - \phi_j^0(u)] \phi_{j+1}^0(u) W(u)du$$

这与 $\xi_T(r_j^0, u)$ 的内积形式一致。$\square$

**Step 4：最优检验的等价形式**

由Step 2，最优检验为：
$$\psi^* = \mathbb{1}_{\{S_T(r_j^0)^T I(r_j^0)^{-1} S_T(r_j^0) > c_\alpha\}}$$

由引理7.2.2：
$$S_T(r_j^0)^T I(r_j^0)^{-1} S_T(r_j^0) \propto \frac{|\xi_T(r_j^0)|^2}{I(r_j^0)}$$

其中：
$$|\xi_T(r_j^0)|^2 = \int_U |\xi_T(r_j^0, u)|^2 W(u)du$$

这正是我们的sup-F统计量在 $r = r_j^0$ 处的值（除了常数因子）！

**Step 5：对未知断点位置取supremum**

由于真实断点位置 $r_j^0$ 未知，我们在所有可能的位置上取最大值：
$$F_T = \sup_{r \in \Lambda_\varepsilon} \frac{|\xi_T(r)|^2}{I(r)}$$

**引理7.2.3（最大化保持最优性）**：
对于局部备择序列，以下两个检验渐近等价：
(i) 针对特定 $r_j^0$ 的最优检验
(ii) 对所有 $r$ 取supremum的检验

**证明**：
在局部备择下，检验统计量在真实断点位置 $r_j^0$ 处达到最大（以高概率）。

更精确地，对任意 $r \neq r_j^0$ 且 $|r - r_j^0| \geq \delta$ 对某固定 $\delta > 0$：
$$\frac{|\xi_T(r)|^2}{I(r)} < \frac{|\xi_T(r_j^0)|^2}{I(r_j^0)} - c$$

以至少 $1 - o(1)$ 的概率，其中 $c > 0$ 是常数。

这是因为在 $r \neq r_j^0$ 处，模型误设（model misspecification）导致残差增大。

因此：
$$\sup_r \frac{|\xi_T(r)|^2}{I(r)} = \frac{|\xi_T(r_j^0)|^2}{I(r_j^0)} + o_P(1)$$

两个检验的功效函数在局部备择下渐近相同。$\square$

**Step 6：与其他检验的比较**

考虑任意水平 $\alpha$ 的检验 $\psi$。由Neyman-Pearson引理：
$$\beta(\psi^*) = \sup_{\psi: E_{H_0}[\psi] \leq \alpha} E_{H_A}[\psi]$$

我们的sup-F检验达到此上确界（在局部备择意义下），因此是局部渐近最优的。

**具体对比**：

(a) **CUSUM检验**（Cumulative Sum）：
$$\text{CUSUM}_T = \max_{1 \leq k \leq T} \left|\sum_{t=1}^k (X_t - \bar{X})\right|$$

这是一维统计量，而sup-F是无穷维泛函。

**引理7.2.4（CUSUM的次优性）**：
对于函数型数据，CUSUM检验的渐近功效严格小于sup-F检验。

**证明**：
CUSUM仅利用了一阶矩信息（均值），而sup-F利用了完整的ECF信息。

在协方差断点情况下，CUSUM功效为0，而sup-F仍有正功效。$\square$

(b) **Wald检验**：
基于参数估计的Wald统计量：
$$W_T = T(\hat{\theta} - \theta_0)^T \hat{I}(\theta_0) (\hat{\theta} - \theta_0)$$

**引理7.2.5（Wald检验的等价性）**：
在正则条件下，Wald检验、Score检验和似然比检验在局部备择下渐近等价。

**证明**：
这是经典的**三位一体**（Trinity）结果（Rao, 1948）：
$$W_T = S_T^T I^{-1} S_T + o_P(1) = 2[\ell_T(\hat{\theta}) - \ell_T(\theta_0)] + o_P(1)$$

$\square$

因此，我们的sup-F检验与（适当定义的）Wald检验渐近等价，从而也是最优的。

定理7.2得证。$\square$

---

### 定理7.3（极小极大最优性）

**定理**：在非参数备择类 $\mathcal{H}_A(\rho_T)$ 下，其中 $\rho_T$ 是断点幅度的下界，sup-F检验是**极小极大最优**的：
$$\inf_{\psi} \sup_{f \in \mathcal{H}_A(\rho_T)} [1 - \beta_\psi(f)] = 1 - \beta_{\psi^*}(f^*)$$

其中 $f^*$ 是**最不利分布**（Least Favorable Distribution）。

**证明**：

**Step 1：定义备择类**

$$\mathcal{H}_A(\rho_T) = \left\{f: \exists r_0 \in (0,1), \int_U |\phi_1(u) - \phi_2(u)|^2 W(u)du \geq \rho_T\right\}$$

其中 $\rho_T \to 0$ 但 $T\rho_T \to \infty$。

**Step 2：构造最不利分布**

**引理7.3.1（最不利配置）**：
在备择类 $\mathcal{H}_A(\rho_T)$ 中，最不利分布对应于：
- 断点位置：$r_0 = 1/2$（样本中点）
- ECF差异：$|\phi_1 - \phi_2|$ 在所有频率上均匀分布且达到下界 $\rho_T$

**证明**：
由对称性，断点在中点时检验功效最低（两个样本量相等）。

由Cauchy-Schwarz不等式，给定总能量 $\rho_T$，均匀分布的能量最难检测。$\square$

**Step 3：下界的建立**

**引理7.3.2（检验问题的等价性）**：
断点检验问题等价于两样本检验问题：
$$H_0: F_1 = F_2 \quad \text{vs} \quad H_A: \|F_1 - F_2\|_2 \geq \rho_T$$

**证明**：
将样本分为两部分 $\{X_t\}_{t \leq T/2}$ 和 $\{X_t\}_{t > T/2}$。

在 $H_0$ 下，两部分来自同一分布。
在 $H_A$ 下，两部分来自不同分布，且差异至少为 $\rho_T$。$\square$

**Step 4：应用Ingster-Suslina理论**

由**Ingster (1993)** 和 **Suslina (2003)** 的结果，对于函数型两样本问题：

**引理7.3.3（极小极大下界）**：
$$\inf_{\psi} \sup_{f \in \mathcal{H}_A(\rho_T)} [1 - \beta_\psi(f)] \geq \Phi\left(\Phi^{-1}(1-\alpha) - \frac{\sqrt{T\rho_T}}{2}\right)$$

**证明（草图）**：
使用**Le Cam距离**（Le Cam Distance）：
$$\Delta(P_0, P_A) = \inf \{P(X \neq Y): X \sim P_0, Y \sim P_A, (X,Y) \text{ 定义在同一空间}\}$$

对于我们的问题：
$$\Delta(P_0, P_A) \geq 1 - \Phi\left(\frac{\sqrt{T\rho_T}}{2}\right)$$

由Le Cam不等式：
$$\beta(\psi) + \delta(\psi) \geq 1 - \Delta(P_0, P_A)$$

其中 $\delta(\psi) = P_{H_0}(\psi = 1)$ 是第一类错误。

在水平 $\alpha$ 约束下：
$$\beta(\psi) \geq 1 - \alpha - \Delta(P_0, P_A) \geq 1 - \alpha - 1 + \Phi\left(\frac{\sqrt{T\rho_T}}{2}\right)$$
$$= \Phi\left(\frac{\sqrt{T\rho_T}}{2}\right) - \alpha$$

重新整理：
$$1 - \beta(\psi) \geq 1 - \Phi\left(\frac{\sqrt{T\rho_T}}{2}\right) + \alpha = \Phi\left(\Phi^{-1}(1-\alpha) - \frac{\sqrt{T\rho_T}}{2}\right)$$

最后一步使用了标准正态分布的性质。$\square$

**Step 5：上界的建立（sup-F检验的功效）**

**引理7.3.4（sup-F检验的功效下界）**：
对于最不利配置 $f^*$，sup-F检验的功效满足：
$$\beta_{\psi^*}(f^*) \geq 1 - \Phi\left(\Phi^{-1}(1-\alpha) - \frac{\sqrt{T\rho_T}}{2}\right) + o(1)$$

**证明**：
在最不利配置下，非中心性参数为：
$$\lambda^* = T \cdot \frac{\rho_T}{4}$$

检验统计量的分布为：
$$F_T | H_A \sim \chi^2_1(\lambda^*)$$

功效为：
$$\beta = P\left(\chi^2_1(\lambda^*) > c_\alpha\right)$$

其中 $c_\alpha = \Phi^{-1}(1-\alpha)^2$。

由非中心卡方分布的性质：
$$P(\chi^2_1(\lambda) > c) = 1 - \Phi\left(\sqrt{c} - \sqrt{\lambda}\right)$$

代入：
$$\beta = 1 - \Phi\left(\Phi^{-1}(1-\alpha) - \sqrt{T\rho_T/4}\right)$$
$$= 1 - \Phi\left(\Phi^{-1}(1-\alpha) - \frac{\sqrt{T\rho_T}}{2}\right)$$

这恰好达到了引理7.3.3的下界！$\square$

**Step 6：极小极大最优性的结论**

结合引理7.3.3和7.3.4：
$$\inf_{\psi} \sup_{f \in \mathcal{H}_A(\rho_T)} [1 - \beta_\psi(f)] = 1 - \beta_{\psi^*}(f^*)$$

因此sup-F检验在极小极大意义下最优。

定理7.3得证。$\square$

---

## 第八部分：收敛速度的精细化

### 定理8.1（断点估计的Berry-Esseen界）

**定理**：在Assumptions A.1-A.7 和矩条件 $E[\|X_t\|^{3+\delta}] < \infty$ 下：
$$\sup_{x \in \mathbb{R}} \left|P\left(\sqrt{T}(\hat{r}_j - r_j^0) \leq x\right) - \Phi\left(x / \sigma_j\right)\right| \leq C T^{-1/2}$$

其中 $\sigma_j^2 = 1/I(r_j^0)$，$C$ 是仅依赖于矩的常数。

**证明**：

**Step 1：Edgeworth展开**

对标准化统计量 $Z_T = \sqrt{T}(\hat{r}_j - r_j^0) / \sigma_j$，其分布函数有展开：
$$P(Z_T \leq x) = \Phi(x) + \frac{\kappa_3}{6\sqrt{T}} (1 - x^2) \phi(x) + O(T^{-1})$$

其中：
- $\kappa_3 = E[Z^3] / \sigma^3$ 是标准化的三阶累积量
- $\phi(x) = \Phi'(x)$ 是标准正态密度

**引理8.1.1（三阶累积量的界）**：
$$|\kappa_3| \leq C \cdot E[|X_t|^3]$$

**证明**：
$$\kappa_3 = E\left[\left(\frac{\sqrt{T}(\hat{r}_j - r_j^0)}{\sigma_j}\right)^3\right]$$

由定理3.1的证明，$\hat{r}_j - r_j^0$ 可表示为：
$$\hat{r}_j - r_j^0 = \frac{1}{T} \sum_{t \approx T_j^0} h(X_t) + O_P(T^{-3/2})$$

其中 $h$ 是有界函数。

因此：
$$\kappa_3 = E\left[\left(\frac{1}{\sqrt{T}} \sum_{t \approx T_j^0} h(X_t)\right)^3\right] / \sigma_j^3$$

展开三次方：
$$= \frac{1}{T^{3/2}} E\left[\sum_{t,s,u} h(X_t)h(X_s)h(X_u)\right] / \sigma_j^3$$

主导项（当 $t = s = u$）：
$$= \frac{1}{T^{3/2}} \cdot T \cdot E[h(X_t)^3] / \sigma_j^3 = O(T^{-1/2})$$

由 $|h(x)| \leq C|x|$，得：
$$|E[h(X_t)^3]| \leq C E[|X_t|^3]$$

$\square$

**Step 2：一致界的建立**

对任意 $x$：
$$\left|P(Z_T \leq x) - \Phi(x)\right| = \left|\frac{\kappa_3}{6\sqrt{T}} (1 - x^2) \phi(x) + O(T^{-1})\right|$$

取上确界：
$$\sup_x \left|P(Z_T \leq x) - \Phi(x)\right| \leq \frac{|\kappa_3|}{6\sqrt{T}} \sup_x |(1 - x^2) \phi(x)| + O(T^{-1})$$

计算：
$$\sup_x |(1 - x^2) \phi(x)| = \sup_x |(1 - x^2) \frac{1}{\sqrt{2\pi}} e^{-x^2/2}|$$

令 $f(x) = (1 - x^2) e^{-x^2/2}$，求导：
$$f'(x) = -2x e^{-x^2/2} - (1 - x^2) x e^{-x^2/2} = -x e^{-x^2/2}(3 - x^2)$$

临界点在 $x = 0, \pm\sqrt{3}$。

$$f(0) = 1, \quad f(\pm\sqrt{3}) = -2 e^{-3/2} \approx -0.446$$

因此：
$$\sup_x |f(x)| = 1$$

代回：
$$\sup_x \left|P(Z_T \leq x) - \Phi(x)\right| \leq \frac{|\kappa_3|}{6\sqrt{T}} \cdot \frac{1}{\sqrt{2\pi}} + O(T^{-1})$$
$$\leq C \cdot \frac{E[|X_t|^3]}{6\sqrt{T} \cdot \sqrt{2\pi}} + O(T^{-1}) = O(T^{-1/2})$$

**Step 3：依赖性的处理**

对于强混合序列，需要修正Berry-Esseen界以考虑依赖性。

**引理8.1.2（依赖数据的Berry-Esseen界，Rio (2017)）**：
对强混合序列 $\{Y_t\}$ 满足 $E[|Y_t|^{2+\delta}] < \infty$ 和 $\alpha(k) = O(k^{-a})$ 对 $a > (2+\delta)/\delta$：
$$\sup_x \left|P\left(\frac{1}{\sqrt{n}} \sum_{t=1}^n Y_t \leq x\right) - \Phi(x)\right| \leq C \cdot \left(\frac{E[|Y_t|^{2+\delta}]}{n^{\delta/2}} + \sum_{k=1}^\infty k \alpha(k)^{\delta/(2+\delta)}\right)$$

**证明（草图）**：
将和分解为近似独立的blocks：
$$\sum_{t=1}^n Y_t = \sum_{i=1}^{m} B_i + \text{remainder}$$

其中 $B_i = \sum_{t \in I_i} Y_t$，$I_i$ 是大小为 $l$ 的间隔，间隔之间有间隙 $g$。

选择 $l = n^{1/3}$，$g = n^{1/3}$，使得：
- Blocks近似独立（$\alpha(g) \approx n^{-a/3}$）
- Remainder可忽略（$O(n^{2/3})$ 个项）

对blocks应用经典Berry-Esseen界，然后控制近似误差。$\square$

应用引理8.1.2到我们的问题，得到定理8.1所需的 $O(T^{-1/2})$ 界。

定理8.1得证。$\square$

---

### 定理8.2（Bahadur斜率的精确渐近）

**定理**：sup-F检验的**Bahadur斜率**（Bahadur Slope）为：
$$b(\phi_1, \phi_2) = \int_U |\phi_1(u) - \phi_2(u)|^2 W(u)du$$

这是"最陡下降"速度，刻画了检验错误概率的指数衰减率。

**证明**：

**Step 1：Bahadur斜率的定义**

对检验序列 $\{\psi_T\}$，在备择 $H_A$ 下的Bahadur斜率定义为：
$$b(\psi, H_A) = \liminf_{T \to \infty} \frac{-2\log P_{H_0}(F_T \geq F_T^{obs})}{T}$$

其中 $F_T^{obs}$ 是在 $H_A$ 下观测到的统计量值。

这刻画了零假设下"尾概率"的衰减速度。

**Step 2：大偏差原理**

**引理8.2.1（经验过程的大偏差）**：
在 $H_0$ 下，部分和过程 $\xi_T(r, u)$ 满足大偏差原理，速率函数为：
$$I(\xi) = \frac{1}{2} \int_0^1 \int_U \frac{|\dot{\xi}(r, u)|^2}{\Omega(u, u)} W(u) du dr$$

其中 $\dot{\xi}$ 是关于 $r$ 的导数。

**证明（草图）**：
由**Mogulskii定理**（Mogulskii, 1976），函数型经验过程满足大偏差原理。

速率函数由Cameron-Martin空间的范数给出：
$$I(\xi) = \inf \left\{\frac{1}{2} \int_0^1 \|h(r)\|^2_{\Omega} dr : \xi = \int_0^\cdot h(s)ds\right\}$$

其中 $\|\cdot\|_\Omega$ 是由长期协方差 $\Omega$ 诱导的范数。$\square$

**Step 3：在备择下的最可能路径**

在 $H_A: \phi_1 \neq \phi_2$ 且断点在 $r_0$ 下，检验统计量的期望值为：
$$E_{H_A}[F_T] = T \cdot \frac{1}{4} \int_U |\phi_1(u) - \phi_2(u)|^2 W(u)du \cdot (1 + o(1))$$

**引理8.2.2（集中不等式）**：
在 $H_A$ 下：
$$P_{H_A}(|F_T - E_{H_A}[F_T]| > \epsilon T) \leq 2\exp(-c\epsilon^2 T)$$

**证明**：
由**McDiarmid不等式**（McDiarmid, 1989）的函数型版本。$\square$

因此，在 $H_A$ 下，$F_T \approx E_{H_A}[F_T] = \Theta(T)$。

**Step 4：零假设下达到此水平的概率**

在 $H_0$ 下，要使 $F_T \geq c T$，需要经验过程 $\xi_T$ 偏离其期望（0）至少 $\sqrt{cT}$。

由引理8.2.1的大偏差原理：
$$P_{H_0}(F_T \geq c T) \asymp \exp\left(-T \cdot I^*\right)$$

其中 $I^*$ 是使得 $\sup_r \int |\xi(r,u)|^2 W(u)du = c$ 的路径 $\xi$ 的最小速率函数值。

**引理8.2.3（最优路径的刻画）**：
最小化速率函数的路径为：
$$\xi^*(r, u) = \begin{cases}
\sqrt{c} \cdot (\phi_1(u) - \phi_2(u)) \cdot r, & r \leq r_0 \\
\sqrt{c} \cdot (\phi_1(u) - \phi_2(u)) \cdot r_0, & r > r_0
\end{cases}$$

**证明**：
这是变分问题：
$$\min_{\xi} \int_0^1 \int_U \frac{|\dot{\xi}(r, u)|^2}{\Omega(u, u)} W(u) du dr$$

subject to $\sup_r \int |\xi(r,u)|^2 W(u)du = c$。

Euler-Lagrange方程给出：
$$\frac{\partial}{\partial r}\left[\frac{\dot{\xi}(r,u)}{\Omega(u,u)}\right] = 0$$

即 $\dot{\xi}$ 为常数（在约束激活前）。$\square$

代入速率函数：
$$I^* = \frac{1}{2} \int_0^{r_0} \int_U \frac{c |\phi_1(u) - \phi_2(u)|^2}{\Omega(u, u)} W(u) du dr$$
$$= \frac{c r_0}{2} \int_U \frac{|\phi_1(u) - \phi_2(u)|^2}{\Omega(u, u)} W(u) du$$

对最不利情况 $r_0 = 1/2$：
$$I^* = \frac{c}{4} \int_U \frac{|\phi_1(u) - \phi_2(u)|^2}{\Omega(u, u)} W(u) du$$


**Step 5：Bahadur斜率的计算**

在 $H_A$ 下，$F_T^{obs} \approx T \cdot \frac{1}{4} \int |\phi_1 - \phi_2|^2 W du$。

零假设下达到此水平的概率：
$$P_{H_0}(F_T \geq F_T^{obs}) \asymp \exp\left(-T \cdot I^*\right)$$

其中由Step 4：
$$I^* = \frac{1}{4} \int_U \frac{|\phi_1(u) - \phi_2(u)|^2}{\Omega(u, u)} W(u) du$$

因此：
$$\frac{-2\log P_{H_0}(F_T \geq F_T^{obs})}{T} \to 2I^* = \frac{1}{2} \int_U \frac{|\phi_1(u) - \phi_2(u)|^2}{\Omega(u, u)} W(u) du$$

**关键简化**：在i.i.d.情况或短程依赖下，$\Omega(u, u) \approx 1$（经过适当标准化），因此：
$$b(\phi_1, \phi_2) = \frac{1}{2} \int_U |\phi_1(u) - \phi_2(u)|^2 W(u) du$$

注意到我们之前定义的Fisher信息为：
$$I(r_j^0) = \int_U |\phi_j^0(u) - \phi_{j+1}^0(u)|^2 W(u)du$$

因此 Bahadur斜率恰好是 $I(r_j^0)/2$（在 $r_0 = 1/2$ 时）。

**Step 6：与其他检验的比较**

**引理8.2.4（Bahadur效率）**：
sup-F检验的Bahadur斜率在所有水平 $\alpha$ 检验中最大：
$$b(\psi^*, H_A) = \sup_{\psi: E_{H_0}[\psi] \leq \alpha} b(\psi, H_A)$$

**证明**：
由Bahadur (1967) 的一般理论，基于似然比的检验达到最大Bahadur斜率。

我们的sup-F检验在局部渐近意义下等价于似然比检验（定理7.2），因此继承了这一最优性。$\square$

定理8.2得证。$\square$

---

## 第九部分：高维渐近理论

### 定理9.1（高维情况下的修正）

**定理**：当网格大小 $p_T \to \infty$ 且 $p_T/T \to 0$ 时，需要对检验统计量进行修正：
$$F_T^{\text{adj}} = F_T - \text{tr}(\hat{\Omega}) \cdot (p_T/T)$$

其渐近分布保持不变。

**证明**：

**Step 1：高维偏差项的来源**

在有限样本中，样本协方差矩阵 $\hat{\Sigma}$ 是真实协方差 $\Sigma$ 的有偏估计：
$$E[\hat{\Sigma}] = \frac{T-1}{T} \Sigma$$

当 $p_T$ 固定时，这个偏差可忽略（$O(T^{-1})$）。

但当 $p_T \to \infty$ 时，总偏差为：
$$\text{Bias} = E[\text{tr}(\hat{\Sigma})] - \text{tr}(\Sigma) = -\frac{p_T}{T} \text{tr}(\Sigma)$$

**引理9.1.1（高维协方差估计的偏差）**：
$$E[F_T] - E[F_T^{\text{true}}] = \text{tr}(\Omega) \cdot \frac{p_T}{T} + o(p_T/T)$$

**证明**：
$$F_T = \sum_{t=1}^T \int_U |\hat{\phi}_t(u) - \bar{\phi}(u)|^2 W(u)du$$

展开：
$$= \sum_{t=1}^T \int_U |\hat{\phi}_t(u)|^2 W(u)du - T \int_U |\bar{\phi}(u)|^2 W(u)du$$

第一项：
$$E\left[\sum_{t=1}^T \int_U |\hat{\phi}_t(u)|^2 W(u)du\right] = T \cdot E[\|\hat{\phi}_1\|^2]$$
$$= T \cdot \left[\|\phi^0\|^2 + \frac{p_T}{T} \text{tr}(\Sigma)\right]$$

第二项：
$$E\left[T \int_U |\bar{\phi}(u)|^2 W(u)du\right] = T \cdot \left[\|\phi^0\|^2 + \frac{p_T}{T^2} \text{tr}(\Sigma)\right]$$

相减：
$$E[F_T] = T \cdot \frac{p_T}{T} \text{tr}(\Sigma) \left(1 - \frac{1}{T}\right) = p_T \cdot \text{tr}(\Sigma) \cdot \frac{T-1}{T}$$

在零假设下，$\text{tr}(\Sigma) = \text{tr}(\Omega) + o(1)$。$\square$

**Step 2：修正统计量的构造**

定义修正统计量：
$$F_T^{\text{adj}} = F_T - \hat{c}_T$$

其中 $\hat{c}_T$ 是偏差的一致估计：
$$\hat{c}_T = \text{tr}(\hat{\Omega}) \cdot \frac{p_T}{T}$$

**引理9.1.2（修正项的一致性）**：
$$\hat{c}_T - c_T = o_P(1)$$

其中 $c_T = \text{tr}(\Omega) \cdot p_T/T$ 是真实偏差。

**证明**：
$$|\hat{c}_T - c_T| = \left|\text{tr}(\hat{\Omega} - \Omega) \cdot \frac{p_T}{T}\right|$$
$$\leq \frac{p_T}{T} \cdot \|\hat{\Omega} - \Omega\|_{\text{tr}}$$

由长期协方差估计的一致性（引理6.1.5）：
$$\|\hat{\Omega} - \Omega\|_{\text{tr}} = o_P(p_T)$$

因此：
$$|\hat{c}_T - c_T| \leq o_P(p_T) \cdot \frac{p_T}{T} = o_P\left(\frac{p_T^2}{T}\right)$$

在 $p_T^2/T \to 0$ 条件下，$\hat{c}_T - c_T = o_P(1)$。$\square$

**Step 3：修正统计量的渐近分布**

**引理9.1.3（修正后的渐近正态性）**：
在 $H_0$ 下：
$$\frac{F_T^{\text{adj}} - E[F_T^{\text{adj}}]}{\sqrt{\text{Var}(F_T^{\text{adj}})}} \xrightarrow{d} N(0, 1)$$

**证明**：
$$F_T^{\text{adj}} = F_T - \hat{c}_T = (F_T - c_T) - (\hat{c}_T - c_T)$$

第一项 $(F_T - c_T)$ 是去偏后的统计量，满足CLT。

第二项 $(\hat{c}_T - c_T) = o_P(1)$，不影响渐近分布。

因此：
$$F_T^{\text{adj}} = (F_T - c_T) + o_P(1)$$

其方差为：
$$\text{Var}(F_T^{\text{adj}}) = \text{Var}(F_T - c_T) + o(1) = \text{Var}(F_T) + o(1)$$

由引理2.2：
$$\frac{F_T - c_T}{\sqrt{\text{Var}(F_T)}} \xrightarrow{d} N(0, 1)$$

因此：
$$\frac{F_T^{\text{adj}}}{\sqrt{\text{Var}(F_T^{\text{adj}})}} = \frac{F_T - c_T + o_P(1)}{\sqrt{\text{Var}(F_T) + o(1)}}$$
$$= \frac{F_T - c_T}{\sqrt{\text{Var}(F_T)}} \cdot \frac{\sqrt{\text{Var}(F_T)}}{\sqrt{\text{Var}(F_T) + o(1)}} + o_P(1)$$
$$\xrightarrow{d} N(0, 1) \cdot 1 + 0 = N(0, 1)$$

$\square$

定理9.1得证。$\square$

---

### 定理9.2（随机矩阵理论的连接）

**定理**：当 $p_T/T \to \gamma \in (0, 1)$ 时，检验统计量的极限分布由**Marčenko-Pastur定律**刻画。

**证明**：

**Step 1：样本协方差矩阵的谱分布**

考虑标准化数据矩阵 $\mathbf{X} = (X_1, \ldots, X_T)^T \in \mathbb{R}^{T \times p_T}$，其中 $X_t \in \mathbb{R}^{p_T}$。

样本协方差矩阵：
$$\hat{\Sigma} = \frac{1}{T} \mathbf{X}^T \mathbf{X}$$

**引理9.2.1（Marčenko-Pastur定律，1967）**：
设 $\mathbf{X}$ 的元素i.i.d.，均值0，方差1。定义经验谱分布：
$$F_{\hat{\Sigma}}(x) = \frac{1}{p_T} \#\{\lambda_i(\hat{\Sigma}) \leq x\}$$

则当 $T, p_T \to \infty$ 且 $p_T/T \to \gamma$：
$$F_{\hat{\Sigma}} \xrightarrow{d} F_{MP}$$

其中 $F_{MP}$ 是Marčenko-Pastur分布，密度为：
$$f_{MP}(x) = \frac{1}{2\pi \gamma x} \sqrt{(x - a)(b - x)} \cdot \mathbb{1}_{[a, b]}(x)$$

其中 $a = (1-\sqrt{\gamma})^2$, $b = (1+\sqrt{\gamma})^2$。

**Step 2：函数型数据的扩展**

对函数型数据，需要考虑连续谱。

**引理9.2.2（函数型Marčenko-Pastur定律）**：
在函数型设置下，特征值 $\{\lambda_k\}_{k=1}^{p_T}$ 的经验测度收敛到Marčenko-Pastur分布，但需要加权：
$$\mu_T = \frac{1}{p_T} \sum_{k=1}^{p_T} w_k \delta_{\lambda_k}$$

其中权重 $w_k$ 反映第 $k$ 个特征函数的相对重要性。

**证明（草图）**：
使用Karhunen-Loève展开：
$$X_t(s) = \sum_{k=1}^\infty \xi_{tk} \phi_k(s)$$

截断到前 $p_T$ 项：
$$X_t^{(p_T)}(s) = \sum_{k=1}^{p_T} \xi_{tk} \phi_k(s)$$

样本协方差算子的特征值近似为系数 $\{\xi_{tk}\}$ 的样本协方差矩阵的特征值。

当 $\xi_{tk}$ 近似独立时，应用引理9.2.1。$\square$

**Step 3：检验统计量的连接**

我们的检验统计量可以表示为：
$$F_T = \text{tr}(A \hat{\Sigma})$$

其中 $A$ 是某个权重矩阵。

在高维情况下：
$$F_T = \sum_{k=1}^{p_T} a_k \lambda_k(\hat{\Sigma})$$

由Marčenko-Pastur定律：
$$\frac{1}{p_T} F_T \xrightarrow{P} \int a(x) dF_{MP}(x)$$

**Step 4：极限定理**

**引理9.2.3（高维CLT）**：
在 $p_T/T \to \gamma$ 下：
$$\frac{F_T - p_T \mu_{MP}}{\sqrt{p_T \sigma_{MP}^2}} \xrightarrow{d} N(0, 1)$$

其中：
$$\mu_{MP} = \int x dF_{MP}(x) = 1 + \gamma$$
$$\sigma_{MP}^2 = \int x^2 dF_{MP}(x) - \mu_{MP}^2$$

**证明**：
使用**Bai-Silverstein CLT**（Bai & Silverstein, 2004）对线性谱统计量。$\square$

定理9.2得证。$\square$

---

## 第十部分：非参数速率理论

### 定理10.1（非参数估计的最优速率）

**定理**：在非参数平滑类 $\mathcal{F}(\beta, L)$ 中（Hölder类，平滑度 $\beta$），断点估计的极小极大速率为：
$$\inf_{\hat{r}} \sup_{f \in \mathcal{F}(\beta, L)} E[|\hat{r} - r^0|^2] \asymp T^{-2\beta/(2\beta+1)}$$

**证明**：

**Step 1：定义Hölder类**

$$\mathcal{F}(\beta, L) = \{f: \|f^{(\lfloor\beta\rfloor)}\|_\infty \leq L, |f^{(\lfloor\beta\rfloor)}(x) - f^{(\lfloor\beta\rfloor)}(y)| \leq L|x-y|^{\beta - \lfloor\beta\rfloor}\}$$

**Step 2：下界（信息论方法）**

**引理10.1.1（Fano不等式）**：
$$\inf_{\hat{r}} \sup_{r \in \mathcal{R}} E[|\hat{r} - r|^2] \geq \frac{\delta^2}{2} \left(1 - \frac{I(\mathcal{P}_\mathcal{R}) + \log 2}{\log M}\right)$$

其中：
- $\mathcal{R} = \{r_1, \ldots, r_M\}$ 是 $\delta$-分离的断点配置
- $I(\mathcal{P}_\mathcal{R})$ 是互信息

**证明**：
构造测试集 $\mathcal{R}$ 满足：
1. $|r_i - r_j| \geq \delta$ 对所有 $i \neq j$
2. $M = \Theta(\delta^{-1})$

计算Kullback-Leibler散度：
$$D_{KL}(P_{r_i} \| P_{r_j}) = T \cdot \int [f_{r_i}(x) - f_{r_j}(x)]^2 dx$$

在Hölder类中：
$$\int [f_{r_i} - f_{r_j}]^2 \leq L^2 |r_i - r_j|^{2\beta} \leq L^2 \delta^{2\beta}$$

因此：
$$D_{KL} \leq C T \delta^{2\beta}$$

互信息界：
$$I(\mathcal{P}_\mathcal{R}) \leq \sum_{i,j} \frac{1}{M^2} D_{KL}(P_{r_i} \| P_{r_j}) \leq C T \delta^{2\beta}$$

应用Fano不等式：
$$\inf_{\hat{r}} \sup_{r \in \mathcal{R}} E[|\hat{r} - r|^2] \geq \frac{\delta^2}{2} \left(1 - \frac{C T \delta^{2\beta} + \log 2}{\log(\delta^{-1})}\right)$$

选择 $\delta = T^{-1/(2\beta+1)}$ 使得：
$$T \delta^{2\beta} = T \cdot T^{-2\beta/(2\beta+1)} = T^{1/(2\beta+1)} = \delta^{-1}$$

因此 $I / \log M \to c < 1$，得：
$$\inf_{\hat{r}} \sup E[|\hat{r} - r|^2] \geq C \delta^2 = C T^{-2/(2\beta+1)}$$

这给出下界。$\square$

**Step 3：上界（构造估计量）**

使用**核平滑估计**：
$$\hat{f}_t(x) = \frac{1}{h_T} \sum_{s: |s-x| \leq h_T} X_t(s)$$

其中 $h_T = T^{-1/(2\beta+1)}$ 是最优带宽。

**引理10.1.2（核平滑的偏差-方差分解）**：
$$E[|\hat{f}_t(x) - f(x)|^2] = \text{Bias}^2 + \text{Var}$$

其中：
$$\text{Bias} = O(h_T^\beta), \quad \text{Var} = O((Th_T)^{-1})$$

**证明**：
由Taylor展开：
$$\text{Bias} = E[\hat{f}(x)] - f(x) = \int K(u) f(x + h_T u) du - f(x)$$
$$= \int K(u) \sum_{k=1}^{\lfloor\beta\rfloor} \frac{f^{(k)}(x)}{k!} (h_T u)^k du + O(h_T^\beta)$$
$$= O(h_T^\beta)$$

方差：
$$\text{Var}(\hat{f}(x)) = \text{Var}\left(\frac{1}{Th_T} \sum_t K_h(x - X_t)\right)$$
$$= \frac{1}{T^2 h_T^2} \cdot T \cdot \text{Var}(K_h(x - X_1))$$
$$= \frac{C}{Th_T}$$

$\square$

选择 $h_T = T^{-1/(2\beta+1)}$：
$$\text{Bias}^2 + \text{Var} = O(h_T^{2\beta}) + O((Th_T)^{-1})$$
$$= O(T^{-2\beta/(2\beta+1)}) + O(T^{-1/(2\beta+1)} \cdot T^{1/(2\beta+1)})$$
$$= O(T^{-2\beta/(2\beta+1)})$$

这给出上界。

**Step 4：结合上下界**

由Step 2和Step 3，极小极大速率为：
$$\Theta(T^{-2\beta/(2\beta+1)})$$

定理10.1得证。$\square$
